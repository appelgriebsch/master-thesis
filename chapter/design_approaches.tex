%!TEX root = ../MasterThesis.tex

\section{Existing System Design Approaches}
\label{sec:system_approaches}

When trying to solve issues of information integration between organisations there are already existing solutions, that have to be examined whether they might fit the E-commerce fraud investigation scenario or not.

\subsection{Web of Services}
\label{subsec:web_services}

With the development of the E-commerce scenario there was also a need to integrate business functionality from various service providers operating on the Internet. Good examples for this are the integration of the \gls{PSP} into the payment as well as the \gls{LSP} for the shipping process. These approaches resulted in the ``Service Oriented Architecture'' paradigm, that enables application services provided by different vendors to talk to each other via a public facing programming interface (aka \gls{API}). The only requirement for such interoperability to work properly is, that each public interface follows some standardised or commonly agreed upon guidelines to be vendor-, platform- as well as language-agnostic. One possible implementation of these concepts are the so-called Web Services, that use the WS* protocols and standards from the \gls{W3C} with the extensible markup language (aka \gls{XML}) and the \gls{HTTP} protocol at their core \citep{josuttis2007soa}. \\

Like the \gls{HTML} format, that is used to represent Web pages on the Internet, \gls{XML} is originally based on \gls{SGML}, but instead of formalising markup tags for structuring and styling textual content it is a meta-language allowing everyone to define his or her own markup languages. In this matter it doesn’t dictate what tags are available to structure the information; instead it includes some basic guidelines for creating wellformed and valid documents that uses domain-specific tags, which can be freely defined and structured by the creator of the XML document. Therefore it is better suited in situations where a computer has to parse and evaluate the content of a message (assuming the computer program knows the structure of the message) \\

In an additional step the author of the API could also specify an XML schema for each message, which describes the structure of the message with all the possible elements, their ordering, nesting level and data types in detail. By doing so the XML parser program can later verify the content of a retrieved message against the XML schema and check if it is a valid document (related to the schema definition). XML schematas are also expressed in XML format and have been standardised by the W3C. Being able to create custom markup languages via XML has a huge benefit for machine-to-machine communication and is the basis for integrating Web Services (via the WS* protocols), but it still has limitations when it comes to figure out the semantics of those XML messages. This is mostly due to the fact that each XML document represents a new markup language and needs a specific XML parser to be understood by the machine; also to distinguish commonly used tag names in an XML document the creator has to place them into specific namespaces (aka XML namespaces). But those XML namespaces further complicate the automatic processing of XML documents and increases the necessity to have custom instances of XML parsers for each XML document \citep{taylor2008p2p}. \\

Mapping the scenario onto the Web of Services approach would mean, that each merchant has to provide a specific \gls{HTTP} \gls{API} endpoint for querying transaction information by the issuer or \gls{PSP}. Additionally each issuer or \gls{PSP} has to collect all available information from the affected merchants (based on recent credit card authorization requests) and combine these various transactional data locally to be able to make an analysis of them that might lead to a decision regarding the E-commerce fraud question from above. \\

This will also mean that each merchant has to provide a separate entrypoint for issuers and \gls{PSP}s to give access to selected information from the internal backend databases. These endpoints have to make safe and secure, so that only allowed parties can access these records. In addition the information from the various internal databases of the merchants have to be converted into a format suitable for external consumption --- in this case a structured \gls{XML} file. \\

Issuers and \gls{PSP}s on the other hand have to maintain a list of \gls{HTTP} \gls{API} endpoints and their respective credentials to access the data from each merchant. Additionally they will have to provide a mapping of each merchant's specific data from the \gls{XML} file to the internal analytics database of the issuer or \gls{PSP}. They also have to trigger internal backend services in case of a suspecious E-commerce transaction that will query the \gls{HTTP} \gls{API} endpoint of the corresponding merchant for further information. \\

As conclusion one could easily see that there are huge efforts on all participating parties to provide information to or integrate data from each other. As there is no common way to access the information at the merchant side (beside the lower level HTTP protocol and XML data format), there have to be a lot of collaboration between each combination of stakeholders for deciding on access patterns and needed data structures in the beginning. Most importantly as each \gls{HTTP} \gls{API} endpoint will be publicly accessible via the Internet it will also opens up a new opportunity for hackers to get access to personal or payment related information.

% subsection web_services (end)

\subsection{Web of Data}
\label{subsec:web_data}

``The Web is full of intelligent applications, with new innovations coming every day'' \citep{allemang2011semantic}. But each of those intelligent Web applications is driven by the data available to them. Data that is likely coming from different places in the global information space — accessible usually via a custom API on the server hosting those resources (see Section~\ref{subsec:web_services}). The more consistent the data available to the smart Web application is the better the service and its result will be. But to support an integration of the data from various Web services the semantics of the information delivered by each service has to be available — and there has to be a generalised, formalised way to express the semantic of that data. The focus on a standard that allows Web services to express the semantics of the data they provide also allows for global scalability, openness and decentralisation, which are the key principles of the World-Wide Web. The Semantic Web tries to give a solution for this problem by providing the Resource Description Framework (aka \gls{RDF}) and related technologies (e.g. RDF schema, SPARQL, OWL, \ldots) for describing, linking and querying the data that a Web service delivers. But it doesn’t reinvent the wheel; instead the Semantic Web builds upon existing, proven technologies like XML, XML namespaces, XML schemata and the \gls{URI} to uniquely address resources on the Web \citep{allemang2011semantic}. \\

A huge benefit of the Web of Data approach is, that the resources delivered are self-describing. They do not only have a consitent and meaningful syntax, but are also semantically self-contained. As of this each merchant has to provide a semantically description of the resources used in a transaction in a standard way --- e.g.\ by using \gls{W3C} standards like \gls{RDF}, \gls{RDFa} or \gls{JSON-LD}. Each merchant also have to provide a \gls{HTTP} \gls{API} endpoint to access and query for the resources, utilizing a query language like \gls{SPARQL}. \\

Each issuer or \gls{PSP} can access these \gls{HTTP} \gls{API} endpoints with her credentials and query for specific information from the public ``information database'' from a merchant. The results of each query can be easily combined into an existing database based on the merging capabilities of \gls{RDF}. The resulting analytic database can be used by the issuer or \gls{PSP} to run queries against or use them with intelligent reasoning tools from the Semantic Web standards for investigation of an E-commerce transaction. \\

The resulting issues and problem are mostly the same as with the Web of Services approach --- beside that the Web of Data offer an unique and integrated way to describe the structure and semantic of the data received from another party. The initial efforts for the implementation of this scenario is also quite high, even if it is lower than with the Web of Services approach. This is mostly due to the fact that there are already some industry-wide and commonly agreed upon ontologies and taxonomies, that are able to describe most of the resources in an E-commerce transaction (e.g.\ GoodRelations Ontology, Schema.org). As it is more likely that merchants do already use them to encode at least some of the data in their backend databases for machine-to-machine communication, it will also descrease the effort on merchant side to provide them for the issuers and \gls{PSP}s. Still these parties have to define the kind of queries and reasoners that might be useful to investigate an E-commerce transaction with the objective to figure out if it is fraudulent or not and have to implement them into their own backend systems.

% subsection web_data (end)

% section system_approaches (end)
