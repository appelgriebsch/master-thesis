%!TEX root = ../MasterThesis.tex

\chapter{Related Works}
\label{cha:related_works}

The study of Pritikana Sen et al.\ starts with an introduction to the subject of \gls{E-commerce} and specifies possible types of it. It further shows the benefits of \gls{E-commerce} (e.g.\ the global reach of Web shops) as well as its limitations. Here it mentions explicitly the security of the system and the communication protocols used. The paper lists the relevant stakeholders of an \gls{E-commerce} transaction and describes the credit card payment process in detail. It concludes with an analysis of the security features of a Web shop and shows that those are not limited to technical aspects alone, but always include the consumers and their behaviours on the Internet \citep{sen2015study}. \\

Sobko's paper defines non-cash transactions and shows ways in which fraudsters try to cheat the system. It starts with a classification of non-cash payments including credit and debit cards that are handed out to individuals by financial institutions. The author also notes possible ways to trick an individual with the objective to get access to credit card information such as phishing and skimming. He explains that once a transaction has been successfully executed with a stolen credit card, the information about it will be sold on the black market to other fraudsters, who will then use the same credit card to make additional purchases. Further on, the paper discusses the impact of fraudulent transactions for the merchants and credit card owners as well as shows technological advances and regulations that have been developed to protect them against non-cash frauds \citep{sobko2014fraud}. \\

The research of Priya J. Rana et al.\ shows possible frauds in \gls{E-commerce} and how they can be detected with current fraud prevention systems. They explain different implementations of fraud detection algorithms, which range from simple rule-based filtering to score-based solutions using fuzzy logic. They conclude that actual systems in use can cover up to 80\% of fraudulent transactions at manageable efforts and costs. More coverage can be achieved by combining existing solutions with information of the card owners' profile, which would introduce credit card usages patterns into the analysis. Still this latter solution is very expensive to implement and operate \citep{rana2015survey}. \\

The paper by Carvalho et al.\ looks into the financial crime investigation process by using banking frauds as an example. It shows that the investigation of them is a very complex task that needs further collaboration between experts. Still just sharing the information will not be sufficient as a common understanding of the different aspects and terms is required. Therefore they state that finding a common language to exchange information is very important for the success of the investigation. Based on this finding the paper also develops an ontology to describe the domain of banking fraud investigation. It elaborates on the objects and their relations in detail and shows that reusing concepts and terms from existing vocabularies can be helpful when designing an ontology of one's own. The authors conclude that semantic technologies can have a positive impact on the crime investigation as they are providing basic reasoning capabilities on the data sets as well as support the combination of information from different sources. Finally they consider semantic technology to be very important in future cybercrime inspections \citep{carvalhoapplying}. \\

The seminal paper ``Linked data-the story so far'' explains the fundamental concepts, approaches and technologies to share data on the Web. It shows how a \gls{RDF} data set should be used to publish structured data on the Web and provide rules how these resources should be described. Additionally it discusses the commonly used vocabularies available on the Web. It also explains ways how to link together different resources on the Internet. After describing different kinds of applications that are possible with the technologies shown, it gives an outlook of future research, which also states the aspects of schema mapping and data fusion as possible challenges. Another issue in the area of Linked data is possible privacy violations that might come up when information from different sources is combined. As conclusion the authors consider the Linked data approach as intermediate step to a Semantic Web, because it also follows the same established Web standards such as \gls{RDF}, \gls{RDFS} and \gls{SPARQL}, but it uses a more pragmatic approach by getting rid of all the complexities involved when having to create, maintain and use large ontologies in \gls{OWL} \citep{bizer2009linked}. \\

Different ways to publish semantic data on the Web have been analysed by Laurens Rietveld et al. They show that current approaches range from simple data dumps of complete \gls{RDF} data sets to publicly available query endpoints using the \gls{SPARQL} protocol and query language. As a conclusion they state that the more flexible approach of \gls{SPARQL} endpoints is generally not working on the Internet, but is instead more suitable for internal data collection and analysis. This is due to the enormous overhead of a \gls{SPARQL} server both in memory and CPU consumption if it has to deal with a large \gls{RDF} data set and a huge number of concurrent users. The proposal, which has been developed in the research paper, is utilizing approaches called ``Linked Data documents'' and ``Triple Pattern Fragments''. The former provides subsets of a \gls{RDF} data set optimized for specific subjects or objects and allow focused querying of a \gls{RDF} data set. The latter tries to move parts of the processing of a large scale \gls{RDF} data set from the server to the clients and builds upon the ideas of the ``Linked Data documents''. As a result of the paper new approaches are needed for offering \gls{RDF} data sets on the Web, which are optimized for querying and processing large scale \gls{RDF} data sets, and make better use of the processing capabilities of clients \citep{rietveld2015linked}. \\

The need for a \gls{RDF} vocabulary to express products and offerings on the Internet was first mentioned and described by Martin Hepp. The author is also the founder of the GoodRelations vocabulary, which he explains in detail in his paper. After showing possible use case scenarios for such an ontology on the Web the author elaborates the available entities and their meanings. Interestingly the author states that he has restricted usage of more expressive \gls{OWL} axioms in the GoodRelations vocabulary due to the limited availability of full-featured \gls{OWL} reasoners. By focusing on \gls{RDFS} constraints only less advanced functionality is required for the processing engine of the \gls{RDF} data sets. The paper closes with examples of using the vocabulary in the \gls{E-commerce} scenario, and its possible future development for \gls{B2B} service integrations \citep{hepp2008goodrelations}. \\

With the wide adoption of semantic data on the Web there was a growing need to harmonise vocabularies used to express commonly shared objects and entities such as events, products, places and people on the Internet. Even though there were initial approaches from the Semantic Web community such as \gls{FOAF} for people, those did not cover every aspect leading search engine providers were looking at to enhance their search results. Due to these circumstances they started an initiative of their own to express and define a meta data vocabulary that covers a wide range of topics discussed on the Web. This initiative resulted in the Schema.org specifications, which due to the influence on search engines such as Google Search, Microsoft Bing or Yahoo! has seen wide adoption by businesses and individuals on the Web. The history of and vision behind this initiative is depicted in the paper from R.V. Guha et al. They also show how the initial idea to embed meta data directly into the \gls{HTML} of a Web site leads to further development of the \gls{RDFa} syntax. The paper closes with an outlook on the future development of Schema.org including the available extension mechanism, which has already been used by certain verticals such as automotive and healthcare \citep{guha2016schema}. \\

The research paper of Christian Vogt et al.\ analyses in how far \gls{WebRTC} data channels can be used to build a large structured \gls{P2P} communication network and exchange arbitrary data on it. The authors describe a multi-layered approach to establish \gls{P2P} connections and route messages through the network. All of their work is done by utilizing the \gls{WebRTC} standard from the \gls{W3C} without requiring additional software on the client side (beside a Web browser with \gls{WebRTC} support). They choose a structured \gls{P2P} network architecture based on a distributed hash-table to speed up managing of and searching for information in the network. They conclude that \gls{WebRTC} as open standard for distributed \gls{P2P} messaging shows a lot of potential for new kinds of applications, and that the standard will need deeper exploration of its potentialities. Additionally they mention the aspects of security and privacy that have to be looked at in detail, but also noticed that there are already groups in the standard councils working on topics such as end-to-end encryption of messages and authentication of peers \citep{vogt2013leveraging}. \\

In another research paper Christian Vogt et al.\ show how to build a distributed content platform on \gls{WebRTC} named BOPlish. Their approach does not rely on any Web server to host and provide access to user generated content, but works by connecting Web browsers directly via \gls{WebRTC} data channels. In this virtual \gls{P2P} content network anyone can easily publish and retrieve information. The proposed application also includes security and authentication features based on \gls{WebRTC} standards, which allow to securely share information between the peers in this \gls{P2P} network. Their proposal still relies on a bootstrap server to setup and connect new peers to the network as well as a distributed hash-table to index and look up available information. A novel approach in the paper is to use custom naming and addressing schema to access and retrieve the information in the \gls{P2P} network. Peers will be authenticated and assigned to a section in a distributed hash-table by the bootstrap server. Information available on a peer can further be addressed directly by the custom addressing scheme, which followed the well-known \gls{URI} scheme, but uses the identity of the owner of the information to address them. When accessing information with this addressing scheme the identity of the owner has to be resolved first, before the information can be retrieved from the actual peer holding them. They conclude that this approach offers decentralized content-publishing between browsers and enables users to share information without the need for a Web server. By providing a new addressing scheme for the content available on the network, which makes accessing the information independent of its current location, their approach is better suited for the dynamics of large \gls{P2P} networks \citep{vogt2013content}. \\

Robert Pienta et al.\ show in their research paper that understanding and making sense out of graph-formatted data is a complex and challenging process, which involves bringing together findings from different research fields such as data mining, machine learning, human computer interaction, and information visualisation. They explain that many of today’s data sets can be represented as graphs, and making sense out of them requires specialized tools that allow users to interactively explore, visualise, and understand large scale graphs. The authors differentiate between two methods to build an understanding of the information within a graph: top-down or global views and bottom-up or local views. The former procedure starts with an overview of the graph as a whole and allows users to filter and zoom in to areas of interest. The latter one focusses on certain aspects of the information and allows users to navigate and analyse data that relates to these aspects. As of this, tools for analysing graph data have to provide different views to support the sensemaking of the information in the graph. Further on, the authors classify the existing techniques based on these two paradigms, and explain each of them briefly. Clustering of a graph, which refers to a method for organising nodes with similar attributes and grouping them together on a global view, can be accomplished with the existing k-SNAP algorithm that also supports drill-down and roll-up operations on the information. For the exploration of a graph, which describes the interactive investigation of it with the intent to develop new insights and is part of the bottom-up approaches of graph investigations, the system can use data mining and machine learning techniques to support the user in the sensemaking process. Especially the available methods for discovering common subgraphs in a larger graph can be helpful in situations that try to find abnormal activities in a network such as frauds \citep{pienta2015scalable}. \\

The paper of Olivia Angiuli et al.\ looks into the subject of privacy issues that come up when large amounts of data about humans are gathered, analysed and eventually shared with others. Due to the fact that analysing and sharing information about humans is subject to laws and regulations, which are specifically designed to protect these subjects, the data has to be anonymised or de-identified prior to making them available. This process should remove certain information from the data set, so that individuals can no longer be identified. It usually starts with an identification of the information that needs to be protected. To de-identify the information in a data set the values can be either generalised into broader categories or contexts, or suppressed from the shared data set completely. They conclude that the process of anonymising data is a complex task that has to take care of any biases, which might be introduced during the process, and also has to prevent the shared information from subsequent re-identification of individuals \citep{angiuli2015identify}.

% chapter related works (end)
